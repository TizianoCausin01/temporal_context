{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38dc054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml, sys\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "# from torchvision.models.feature_extraction import (\n",
    "#     create_feature_extractor,\n",
    "#     get_graph_node_names,\n",
    "# )\n",
    "import torch \n",
    "import joblib\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "ENV = os.getenv(\"MY_ENV\", \"dev\")\n",
    "with open(\"../../config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "paths = config[ENV][\"paths\"]\n",
    "sys.path.append(paths[\"src_path\"])\n",
    "from general_utils.utils import print_wise, get_layer_output_shape, get_device\n",
    "from image_processing.utils import read_video, resize_video_array, load_torchvision_model\n",
    "from image_processing.computational_models import compute_torchvision_model\n",
    "device=get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b5579",
   "metadata": {},
   "source": [
    "The core function should:\n",
    "- 1 read the video\n",
    "- 2 make it suitable for the model\n",
    "- 3 do the forward pass\n",
    "- 4 project onto the PCs\n",
    "- 5 save it\n",
    "\n",
    "The func to pass to ```master_workers_queue``` (parallelize over the layers) should:\n",
    "- download the PCs for different videotypes \n",
    "- run the core function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee7eb7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.931645086500794e-05 0.1169830709695816\n"
     ]
    }
   ],
   "source": [
    "model_name = 'alexnet'\n",
    "layer_name = 'features.0'\n",
    "model = load_torchvision_model(model_name, device)\n",
    "p = next(model.parameters())\n",
    "print(p.mean().item(), p.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e819e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0\n",
    "max_len = 20\n",
    "device = get_device()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a80ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tizianocausin/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator IncrementalPCA from version 1.8.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_IMG_4668.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_IMG_4697.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_YDXJ0100A.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_YDXJ0100.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_rubin1_to_LiFeng.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_rubin_to_venus_10s_rev.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_rubin1_to_roscoe.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_IMG_4708.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_IMG_4709.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_IMG_4721.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_anna1_to_blondie.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_anna2_to_paul_10s.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_YDXJ0090A.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_YDXJ0086A.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_stevetoB5.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_IMG_4696.npz\n",
      "11:42:35 - rank 0 model already exists at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_girl1_to_rubin2_10s_rev.npz\n",
      "11:42:36 - rank 0 IMG_4669.mp4 read successfully\n",
      "11:42:39 - rank 0 model of size (1000, 346) saved at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_IMG_4669.npz\n",
      "11:42:40 - rank 0 IMG_4655.mp4 read successfully\n",
      "11:42:42 - rank 0 model of size (1000, 323) saved at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_IMG_4655.npz\n",
      "11:42:43 - rank 0 IMG_4657.mp4 read successfully\n",
      "11:42:45 - rank 0 model of size (1000, 299) saved at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_IMG_4657.npz\n",
      "11:42:46 - rank 0 anna1_10s.mp4 read successfully\n",
      "11:42:48 - rank 0 model of size (1000, 311) saved at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_anna1_10s.npz\n",
      "11:42:49 - rank 0 YDXJ0100B.MP4 read successfully\n",
      "11:42:51 - rank 0 model of size (1000, 307) saved at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_YDXJ0100B.npz\n",
      "11:42:52 - rank 0 IMG_4694.mp4 read successfully\n",
      "11:42:54 - rank 0 model of size (1000, 322) saved at /Users/tizianocausin/livingstone_lab_local/tiziano/models/alexnet_features.0_IMG_4694.npz\n",
      "11:42:56 - rank 0 YDXJ0088.MP4 read successfully\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcompute_torchvision_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca_opt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/temporal_context/python_scripts/src/image_processing/computational_models.py:425\u001b[39m, in \u001b[36mcompute_torchvision_model\u001b[39m\u001b[34m(paths, rank, layer_name, model_name, model, device, max_len, pca_opt)\u001b[39m\n\u001b[32m    423\u001b[39m     curr_ipca = ipca_faceswap\n\u001b[32m    424\u001b[39m \u001b[38;5;66;03m# end if \"YDX\" in fn:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m feats = \u001b[43mpass_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m feats_proj = feats @ curr_ipca.components_.T \u001b[38;5;28;01mif\u001b[39;00m pca_opt \u001b[38;5;28;01melse\u001b[39;00m feats\n\u001b[32m    427\u001b[39m feats_proj = feats_proj.T\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/temporal_context/python_scripts/src/image_processing/computational_models.py:376\u001b[39m, in \u001b[36mpass_video\u001b[39m\u001b[34m(paths, rank, feature_extractor, layer_name, fn, device, max_len, new_height, new_width)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpass_video\u001b[39m(paths, rank, feature_extractor, layer_name, fn, device, max_len=\u001b[32m20\u001b[39m, new_height=\u001b[32m224\u001b[39m, new_width=\u001b[32m224\u001b[39m):\n\u001b[32m    375\u001b[39m     video = read_video(paths, rank, fn, vid_duration=max_len)\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     inputs = \u001b[43mresize_video_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m     inputs = torch.from_numpy(inputs).float().to(device)\n\u001b[32m    378\u001b[39m     inputs = inputs.permute(\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/temporal_context/python_scripts/src/image_processing/utils.py:84\u001b[39m, in \u001b[36mresize_video_array\u001b[39m\u001b[34m(video, new_height, new_width, interpolation, normalize)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[32m     83\u001b[39m     mean = resized_video.mean(axis=(\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     std = \u001b[43mresized_video\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m + \u001b[32m1e-8\u001b[39m\n\u001b[32m     85\u001b[39m     resized_video = (resized_video - mean) / std\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# end if normalize:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/numpy/core/_methods.py:206\u001b[39m, in \u001b[36m_std\u001b[39m\u001b[34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_std\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, ddof=\u001b[32m0\u001b[39m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m, *,\n\u001b[32m    205\u001b[39m          where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     ret = \u001b[43m_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[43m=\u001b[49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m               \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu.ndarray):\n\u001b[32m    210\u001b[39m         ret = um.sqrt(ret, out=ret)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/numpy/core/_methods.py:152\u001b[39m, in \u001b[36m_var\u001b[39m\u001b[34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[39m\n\u001b[32m    147\u001b[39m     dtype = mu.dtype(\u001b[33m'\u001b[39m\u001b[33mf8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# Compute the mean.\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# Note that if dtype is not of inexact type then arraymean will\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# not be either.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m arrmean = \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# The shape of rcount has to match arrmean to not change the shape of out\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# in broadcasting. Otherwise, it cannot be stored back to arrmean.\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rcount.ndim == \u001b[32m0\u001b[39m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;66;03m# fast-path for default case when where is True\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "compute_torchvision_model(paths, rank, layer_name, model_name, model, device, pca_opt=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temporal_context",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
