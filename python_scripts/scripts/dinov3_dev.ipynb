{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c3a4469-b031-494c-9606-af003ea69423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THE LARGE ONE!!!\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "token = os.getenv(\"GITHUB_TOKEN\")\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b77990-5c13-40f9-a5ba-31be801cfee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0103 19:31:48.821000 39273 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import os, yaml, sys\n",
    "import torch\n",
    "from einops import reduce, rearrange\n",
    "from transformers import pipeline\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "from huggingface_hub import login\n",
    "token = os.getenv(\"GITHUB_TOKEN\")\n",
    "login(token=token)\n",
    "pipe = pipeline(\n",
    "    task=\"image-feature-extraction\",\n",
    "    model=\"facebook/dinov3-vits16-pretrain-lvd1689m\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "f = pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96fb3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:31:57 - device being used: mps\n"
     ]
    }
   ],
   "source": [
    "ENV = os.getenv(\"MY_ENV\", \"dev\")\n",
    "with open(\"../../config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "paths = config[ENV][\"paths\"]\n",
    "sys.path.append(paths[\"src_path\"])\n",
    "from general_utils.utils import get_device, get_module_by_path\n",
    "from image_processing.utils import get_activation\n",
    "device = get_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfad5053-a0f8-4677-b9b0-cb6b02938dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled output shape: torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from transformers.image_utils import load_image\n",
    "import accelerate\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = load_image(url)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov3-vits16-pretrain-lvd1689m\")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"facebook/dinov3-vits16-pretrain-lvd1689m\",\n",
    "    dtype=torch.float16,\n",
    "    attn_implementation=\"sdpa\"\n",
    ").to(device)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "pooled_output = outputs.pooler_output\n",
    "print(\"Pooled output shape:\", pooled_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d65b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77184])\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store intermediate features\n",
    "features = {}\n",
    "layer_name = \"layer.0.mlp.down_proj\"\n",
    "module = get_module_by_path(model, layer_name)\n",
    "module._forward_hooks.clear()\n",
    "handle = module.register_forward_hook(get_activation(layer_name, features, 'all'))\n",
    "\n",
    "# Run forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "# Access features\n",
    "print(features[layer_name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6739e8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "# m = model.layer[0].mlp.down_proj\n",
    "# m._forward_hooks.clear()\n",
    "# print(m._forward_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af562853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_adjust_missing_and_unexpected_keys',\n",
       " '_apply',\n",
       " '_auto_class',\n",
       " '_backward_compatibility_gradient_checkpointing',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_can_compile_fullgraph',\n",
       " '_can_record_outputs',\n",
       " '_can_set_attn_implementation',\n",
       " '_check_and_adjust_attn_implementation',\n",
       " '_checkpoint_conversion_mapping',\n",
       " '_compiled_call_impl',\n",
       " '_copy_lm_head_original_to_resized',\n",
       " '_create_repo',\n",
       " '_dispatch_accelerate_model',\n",
       " '_ep_plan',\n",
       " '_fix_state_dict_key_on_load',\n",
       " '_fix_state_dict_key_on_save',\n",
       " '_fix_state_dict_keys_on_save',\n",
       " '_flash_attn_2_can_dispatch',\n",
       " '_flash_attn_3_can_dispatch',\n",
       " '_flex_attn_can_dispatch',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_from_config',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_files_timestamps',\n",
       " '_get_key_renaming_mapping',\n",
       " '_get_name',\n",
       " '_get_no_split_modules',\n",
       " '_get_resized_embeddings',\n",
       " '_get_resized_lm_head',\n",
       " '_hf_peft_config_loaded',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_added_embeddings_weights_with_mean',\n",
       " '_init_added_lm_head_bias_with_mean',\n",
       " '_init_added_lm_head_weights_with_mean',\n",
       " '_init_weights',\n",
       " '_initialize_missing_keys',\n",
       " '_initialize_weights',\n",
       " '_input_embed_layer',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_is_stateful',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keep_in_fp32_modules_strict',\n",
       " '_keep_in_fp32_modules_strict',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_state_dict',\n",
       " '_load_pretrained_model',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_move_missing_keys_from_meta_to_cpu',\n",
       " '_named_members',\n",
       " '_no_split_modules',\n",
       " '_no_split_modules',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_pp_plan',\n",
       " '_pp_plan',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_save_to_state_dict',\n",
       " '_sdpa_can_dispatch',\n",
       " '_set_default_dtype',\n",
       " '_set_gradient_checkpointing',\n",
       " '_skip_keys_device_placement',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_supports_attention_backend',\n",
       " '_supports_flash_attn',\n",
       " '_supports_flex_attn',\n",
       " '_supports_sdpa',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_tied_weights_keys',\n",
       " '_tp_plan',\n",
       " '_tp_plan',\n",
       " '_tp_size',\n",
       " '_upload_modified_files',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'active_adapters',\n",
       " 'add_adapter',\n",
       " 'add_memory_hooks',\n",
       " 'add_model_tags',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'can_generate',\n",
       " 'can_record_outputs',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'cpu',\n",
       " 'create_extended_attention_mask_for_decoder',\n",
       " 'cuda',\n",
       " 'delete_adapter',\n",
       " 'dequantize',\n",
       " 'device',\n",
       " 'disable_adapters',\n",
       " 'disable_input_require_grads',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'embeddings',\n",
       " 'enable_adapters',\n",
       " 'enable_input_require_grads',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'generation_config',\n",
       " 'get_adapter_state_dict',\n",
       " 'get_buffer',\n",
       " 'get_compiled_call',\n",
       " 'get_correct_attn_implementation',\n",
       " 'get_decoder',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_extra_state',\n",
       " 'get_init_context',\n",
       " 'get_input_embeddings',\n",
       " 'get_memory_footprint',\n",
       " 'get_output_embeddings',\n",
       " 'get_parameter',\n",
       " 'get_parameter_or_buffer',\n",
       " 'get_position_embeddings',\n",
       " 'get_submodule',\n",
       " 'gradient_checkpointing',\n",
       " 'gradient_checkpointing_disable',\n",
       " 'gradient_checkpointing_enable',\n",
       " 'half',\n",
       " 'init_weights',\n",
       " 'initialize_weights',\n",
       " 'invert_attention_mask',\n",
       " 'ipu',\n",
       " 'is_backend_compatible',\n",
       " 'is_gradient_checkpointing',\n",
       " 'kernelize',\n",
       " 'layer',\n",
       " 'load_adapter',\n",
       " 'load_state_dict',\n",
       " 'loss_function',\n",
       " 'loss_type',\n",
       " 'main_input_name',\n",
       " 'model_tags',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'name_or_path',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'norm',\n",
       " 'num_parameters',\n",
       " 'parameters',\n",
       " 'post_init',\n",
       " 'pp_plan',\n",
       " 'prune_heads',\n",
       " 'push_to_hub',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_for_auto_class',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_position_embeddings',\n",
       " 'resize_token_embeddings',\n",
       " 'retrieve_modules_from_names',\n",
       " 'reverse_bettertransformer',\n",
       " 'rope_embeddings',\n",
       " 'save_pretrained',\n",
       " 'set_adapter',\n",
       " 'set_attn_implementation',\n",
       " 'set_decoder',\n",
       " 'set_extra_state',\n",
       " 'set_input_embeddings',\n",
       " 'set_output_embeddings',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'smart_apply',\n",
       " 'state_dict',\n",
       " 'supports_gradient_checkpointing',\n",
       " 'supports_pp_plan',\n",
       " 'supports_tp_plan',\n",
       " 'tie_embeddings_and_encoder_decoder',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'to_bettertransformer',\n",
       " 'to_empty',\n",
       " 'tp_plan',\n",
       " 'tp_size',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'use_kernels',\n",
       " 'warn_if_padding_and_no_attention_mask',\n",
       " 'warnings_issued',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f70d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: 'embeddings' (DINOv3ViTEmbeddings)\n",
      "  param: cls_token            shape=(1, 1, 384)          numel=       384 dtype=torch.float16\n",
      "  param: mask_token           shape=(1, 1, 384)          numel=       384 dtype=torch.float16\n",
      "  param: register_tokens      shape=(1, 4, 384)          numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'embeddings.patch_embeddings' (Conv2d)\n",
      "  param: weight               shape=(384, 3, 16, 16)     numel=    294912 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'norm' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Total params: 21,596,544\n",
      "Trainable params: 21,596,544\n"
     ]
    }
   ],
   "source": [
    "# Print model modules and parameter shapes (uses existing `model` and `torch` from the notebook)\n",
    "def print_model_layer_shapes(model):\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "\n",
    "    for module_name, module in model.named_modules():\n",
    "        # skip the top-level empty name unless it has no parent info to show\n",
    "        params = list(module.named_parameters(recurse=False))\n",
    "        if not params:\n",
    "            continue\n",
    "\n",
    "        print(f\"Module: '{module_name}' ({module.__class__.__name__})\")\n",
    "        for p_name, p in params:\n",
    "            shape = tuple(p.shape)\n",
    "            numel = p.numel()\n",
    "            total_params += numel\n",
    "            if p.requires_grad:\n",
    "                trainable_params += numel\n",
    "            print(f\"  param: {p_name:20s} shape={str(shape):20s} numel={numel:10d} dtype={p.dtype}\")\n",
    "        print()\n",
    "\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {trainable_params:,}\")\n",
    "\n",
    "# Run the printer\n",
    "print_model_layer_shapes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c9df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store features\n",
    "features = {}\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    features[\"layer0_scale1\"] = output.detach()\n",
    "\n",
    "# Register hook\n",
    "target_layer = model.layer[0].layer_scale1\n",
    "hook_handle = target_layer.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bee73e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(inputs['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hooked feature shape: torch.Size([1, 201, 384])\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.inference_mode():    \n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the saved feature\n",
    "feat = features[\"layer0_scale1\"]\n",
    "print(\"Hooked feature shape:\", feat.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temporal_context",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
