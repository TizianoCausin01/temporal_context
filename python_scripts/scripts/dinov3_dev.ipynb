{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c3a4469-b031-494c-9606-af003ea69423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "token = os.getenv(\"GITHUB_TOKEN\")\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b77990-5c13-40f9-a5ba-31be801cfee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1213 18:46:29.955000 69009 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import os, yaml, sys\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "from huggingface_hub import login\n",
    "token = os.getenv(\"GITHUB_TOKEN\")\n",
    "login(token=token)\n",
    "pipe = pipeline(\n",
    "    task=\"image-feature-extraction\",\n",
    "    model=\"facebook/dinov3-vits16-pretrain-lvd1689m\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "f = pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfad5053-a0f8-4677-b9b0-cb6b02938dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled output shape: torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from transformers.image_utils import load_image\n",
    "import accelerate\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = load_image(url)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov3-vits16-pretrain-lvd1689m\")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"facebook/dinov3-vits16-pretrain-lvd1689m\",\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\"\n",
    ")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "pooled_output = outputs.pooler_output\n",
    "print(\"Pooled output shape:\", pooled_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87160c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tizianocausin/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/nw/yf48zdjj1m5012281wx_pzhc0000gn/T/ipykernel_69009/3213110985.py\", line 5, in <module>\n",
      "    get_graph_node_names(model)\n",
      "  File \"/Users/tizianocausin/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/torchvision/models/feature_extraction.py\", line 257, in get_graph_node_names\n",
      "    train_tracer.trace(model.train(), concrete_args=concrete_args)\n",
      "  File \"/Users/tizianocausin/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py\", line 850, in trace\n",
      "    (self.create_arg(fn(*args)),),\n",
      "                     ^^^^^^^^^\n",
      "  File \"/Users/tizianocausin/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/transformers/utils/generic.py\", line 826, in wrapper\n",
      "    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n",
      "        ^\n",
      "TypeError: 'Proxy' object does not support item assignment\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tizianocausin/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 2194, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tizianocausin/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1179, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tizianocausin/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1050, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tizianocausin/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 858, in structured_traceback\n",
      "    formatted_exceptions: list[list[str]] = self.format_exception_as_a_whole(\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tizianocausin/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 770, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tizianocausin/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 533, in format_record\n",
      "    assert isinstance(frame_info.lineno, int)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "# the usual feature extractor doesn't work\n",
    "from torchvision.models.feature_extraction import (\n",
    "    create_feature_extractor,\n",
    "    get_graph_node_names,\n",
    ")\n",
    "get_graph_node_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f70d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: 'embeddings' (DINOv3ViTEmbeddings)\n",
      "  param: cls_token            shape=(1, 1, 384)          numel=       384 dtype=torch.float16\n",
      "  param: mask_token           shape=(1, 1, 384)          numel=       384 dtype=torch.float16\n",
      "  param: register_tokens      shape=(1, 4, 384)          numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'embeddings.patch_embeddings' (Conv2d)\n",
      "  param: weight               shape=(384, 3, 16, 16)     numel=    294912 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.0.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.1.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.2.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.3.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.4.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.5.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.6.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.7.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.8.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.9.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.10.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.norm1' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.attention.k_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.attention.v_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.attention.q_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.attention.o_proj' (Linear)\n",
      "  param: weight               shape=(384, 384)           numel=    147456 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.layer_scale1' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.norm2' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.mlp.up_proj' (Linear)\n",
      "  param: weight               shape=(1536, 384)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(1536,)              numel=      1536 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.mlp.down_proj' (Linear)\n",
      "  param: weight               shape=(384, 1536)          numel=    589824 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'layer.11.layer_scale2' (DINOv3ViTLayerScale)\n",
      "  param: lambda1              shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Module: 'norm' (LayerNorm)\n",
      "  param: weight               shape=(384,)               numel=       384 dtype=torch.float16\n",
      "  param: bias                 shape=(384,)               numel=       384 dtype=torch.float16\n",
      "\n",
      "Total params: 21,596,544\n",
      "Trainable params: 21,596,544\n"
     ]
    }
   ],
   "source": [
    "# Print model modules and parameter shapes (uses existing `model` and `torch` from the notebook)\n",
    "def print_model_layer_shapes(model):\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "\n",
    "    for module_name, module in model.named_modules():\n",
    "        # skip the top-level empty name unless it has no parent info to show\n",
    "        params = list(module.named_parameters(recurse=False))\n",
    "        if not params:\n",
    "            continue\n",
    "\n",
    "        print(f\"Module: '{module_name}' ({module.__class__.__name__})\")\n",
    "        for p_name, p in params:\n",
    "            shape = tuple(p.shape)\n",
    "            numel = p.numel()\n",
    "            total_params += numel\n",
    "            if p.requires_grad:\n",
    "                trainable_params += numel\n",
    "            print(f\"  param: {p_name:20s} shape={str(shape):20s} numel={numel:10d} dtype={p.dtype}\")\n",
    "        print()\n",
    "\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {trainable_params:,}\")\n",
    "\n",
    "# Run the printer\n",
    "print_model_layer_shapes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c9df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store features\n",
    "features = {}\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    features[\"layer0_scale1\"] = output.detach()\n",
    "\n",
    "# Register hook\n",
    "target_layer = model.layer[0].layer_scale1\n",
    "hook_handle = target_layer.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bee73e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(inputs['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hooked feature shape: torch.Size([1, 201, 384])\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.inference_mode():    \n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the saved feature\n",
    "feat = features[\"layer0_scale1\"]\n",
    "print(\"Hooked feature shape:\", feat.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temporal_context",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
