{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml, sys\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from torchvision.models.feature_extraction import (\n",
    "    create_feature_extractor,\n",
    "    get_graph_node_names,\n",
    ")\n",
    "import torch \n",
    "\n",
    "ENV = os.getenv(\"MY_ENV\", \"dev\")\n",
    "with open(\"../../config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "paths = config[ENV][\"paths\"]\n",
    "sys.path.append(paths[\"src_path\"])\n",
    "from general_utils.utils import print_wise\n",
    "from image_processing.utils import read_video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc2c238",
   "metadata": {},
   "source": [
    "## Steps\n",
    "(parallel over the layers of the video)\n",
    "1. Load videos until you get approx 2 or 3 times the batch-size (or more?) (for very long vids (some of the ones with arcaro for instance - but check), just take the first 20 seconds or so...) (can we estimate the optimal order based on get_video_dimensions?) -> then shuffle, split evenly the frames and pass it to iPCA \n",
    "2. Normalize, format and shuffle them\n",
    "    - If gaze dependent, load gaze, upsample (in time) video and extract gaze-dep spatial window\n",
    "3. Compute iPCA\n",
    "4. Save eigenvectors and eigenvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73b4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f6d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_layer_out_shape\n",
    "Computes the output shape (excluding batch size) of a specific layer \n",
    "from a given PyTorch feature extractor when applied to a dummy input \n",
    "image of size (1, 3, 224, 224).\n",
    "INPUT:\n",
    "- feature_extractor: torch.nn.Module -> A PyTorch model (typically a feature extractor created via torchvision.models.feature_extraction.create_feature_extractor)\n",
    "                                        which outputs a dictionary of intermediate activations.\n",
    "            \n",
    "- layer_name: str -> The name of the layer for which the output shape is desired. This must be one of the keys returned by the feature_extractor.\n",
    "\n",
    "OUTPUT:\n",
    "- tmp_shape: Tuple(Int) -> A tuple representing the shape of the output tensor from the specified layer, excluding the batch dimension. For example,\n",
    "                          (512, 7, 7) for a convolutional layer or (768,) for a transformer block.\n",
    "            \n",
    "Example Usage:\n",
    "    >>> from torchvision.models import resnet18\n",
    "    >>> from torchvision.models.feature_extraction import create_feature_extractor\n",
    "    >>> model = resnet18(pretrained=True).eval()\n",
    "    >>> feat_ext = create_feature_extractor(model, return_nodes=[\"layer1.0.relu_1\"])\n",
    "    >>> shape = get_layer_out_shape(feat_ext, \"layer1.0.relu_1\")\n",
    "    >>> print(shape)\n",
    "    (64, 56, 56)\n",
    "\"\"\"\n",
    "def get_layer_output_shape(feature_extractor, layer_name):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # thi if leave it here or not...\n",
    "    with torch.no_grad():\n",
    "        in_proxy = torch.randn(1, 3, 224, 224).to(device)\n",
    "        tmp_shape = feature_extractor(in_proxy)[layer_name].shape[1:]\n",
    "    return tmp_shape\n",
    "\n",
    "\n",
    "def ipca_core(paths, rank, layer_name, model_name, n_components, model, loader, device):\n",
    "    save_name = (f\"imagenet_val_{model_name}_{layer_name}_pca_model_{n_components}_PCs.pkl\")\n",
    "    path = os.path.join(paths[\"results_path\"], save_name)\n",
    "    if os.path.exists(path):\n",
    "        print_wise(f\"{path} already exists\")\n",
    "    else:\n",
    "        print_wise(f\"Fitting PCA for layer: {layer_name}\", rank=rank)\n",
    "        feature_extractor = create_feature_extractor(\n",
    "            model, return_nodes=[layer_name]\n",
    "        ).to(device)\n",
    "        tmp_shape = get_layer_output_shape(feature_extractor, layer_name)\n",
    "        n_features = np.prod(tmp_shape)  # [C, H, W] -> C*H*W\n",
    "        n_components_layer = min(n_features, n_components)  # Limit to number of features\n",
    "        pca = IncrementalPCA(n_components=n_components_layer)\n",
    "        counter = 0\n",
    "        for inputs, _ in loader:\n",
    "            counter += 1\n",
    "            print_wise(f\"starting batch {counter}\", rank=rank)\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device)\n",
    "                feats = feature_extractor(inputs)[layer_name]\n",
    "                feats = feats.view(feats.size(0), -1).cpu().numpy()\n",
    "                pca.partial_fit(feats)\n",
    "\n",
    "        joblib.dump(pca, path) # better this or pkl?\n",
    "        print_wise(f\"Saved PCA for {layer_name} at {path}\", rank=rank)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temporal_context",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
