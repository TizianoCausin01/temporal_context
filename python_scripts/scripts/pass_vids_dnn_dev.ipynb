{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml, sys\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from torchvision.models.feature_extraction import (\n",
    "    create_feature_extractor,\n",
    "    get_graph_node_names,\n",
    ")\n",
    "import torch \n",
    "import cv2\n",
    "import joblib\n",
    "\n",
    "ENV = os.getenv(\"MY_ENV\", \"dev\")\n",
    "with open(\"../../config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "paths = config[ENV][\"paths\"]\n",
    "sys.path.append(paths[\"src_path\"])\n",
    "from general_utils.utils import print_wise, get_layer_output_shape, get_device\n",
    "from image_processing.utils import concatenate_frames_batch, shuffle_frames, list_videos, get_frames_number, split_in_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc2c238",
   "metadata": {},
   "source": [
    "## Steps\n",
    "(parallel over the layers of the video)\n",
    "1. Load videos until you get approx 2 or 3 times the batch-size (or more?) (for very long vids (some of the ones with arcaro for instance - but check), just take the first 20 seconds or so...) (can we estimate the optimal order based on get_video_dimensions?) -> then shuffle, split evenly the frames and pass it to iPCA \n",
    "2. Normalize, format and shuffle them\n",
    "    - If gaze dependent, load gaze, upsample (in time) video and extract gaze-dep spatial window\n",
    "3. Compute iPCA\n",
    "4. Save eigenvectors and eigenvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d6b11e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tizianocausin/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "n_components = 100\n",
    "device = get_device()\n",
    "model_name = \"alexnet\"\n",
    "layer_name = \"features.7\"\n",
    "model_cls = getattr(models, model_name)\n",
    "model = model_cls(weights=True).to(device).eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ipca_videos(paths, rank, layer_name, model_name, model, n_components, video_type, batches_to_proc, batch_sizes, fn_list, long_vids,  device, vid_duration_lim, new_h=224, new_w=224):\n",
    "    save_name = (f\"{video_type}_{model_name}_{layer_name}_ipca_{n_components}_PCs.pkl\")\n",
    "    path = os.path.join(f\"{paths[\"livingstone_lab\"]}/tiziano/models\", save_name)\n",
    "    if os.path.exists(path):\n",
    "        print_wise(f\"{path} already exists\")\n",
    "    else:\n",
    "        print_wise(f\"Fitting PCA for layer: {layer_name}\", rank=rank)\n",
    "        frames_batch = []\n",
    "        feature_extractor = create_feature_extractor(\n",
    "            model, return_nodes=[layer_name]\n",
    "        ).to(device)\n",
    "        tmp_shape = get_layer_output_shape(feature_extractor, layer_name)\n",
    "        n_features = np.prod(tmp_shape)  # [C, H, W] -> C*H*W\n",
    "        n_components_layer = min(n_features, n_components)  # Limit to number of features\n",
    "        ipca = IncrementalPCA(n_components=n_components_layer, batch_size=batch_sizes[0])\n",
    "        curr_video_idx = 0\n",
    "        for idx, curr_batch_size in enumerate(batch_sizes):\n",
    "\n",
    "            print_wise(f\"starting batch {idx}\", rank=rank)\n",
    "            frames_batch, curr_video_idx = concatenate_frames_batch(paths, rank, fn_list, frames_batch, curr_video_idx, idx, batches_to_proc, batch_sizes, new_h, new_w, long_vids, vid_duration_lim)\n",
    "            frames_batch = shuffle_frames(frames_batch)\n",
    "            inputs = frames_batch[:curr_batch_size, :, :, :]\n",
    "            inputs = torch.from_numpy(inputs).float().to(device)\n",
    "            inputs = inputs.permute(0, 3, 1, 2)\n",
    "            frames_batch = frames_batch[curr_batch_size:, :, :, :]\n",
    "            with torch.no_grad():\n",
    "                feats = feature_extractor(inputs)[layer_name]\n",
    "                print(\"feats\", feats.shape)\n",
    "                feats = feats.view(feats.size(0), -1).cpu().numpy()\n",
    "            ipca.partial_fit(feats)\n",
    "            # end with torch.no_grad():\n",
    "        \n",
    "        #ADD FIX UNCOMMENT THIS LATER !!!!!!!\n",
    "        #joblib.dump(ipca, path) # better this or pkl?\n",
    "        print_wise(f\"Saved PCA for {layer_name} at {path}\", rank=rank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cda333d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:16:02 - rank 0 Fitting PCA for layer: features.7\n",
      "12:16:02 - rank 0 starting batch 0\n",
      "12:16:03 - rank 0 YDXJ0100A.MP4 read successfully\n",
      "12:16:06 - rank 0 YDXJ0100.MP4 read successfully\n",
      "12:16:08 - rank 0 YDXJ0090A.MP4 read successfully\n",
      "12:16:09 - rank 0 YDXJ0086A.MP4 read successfully\n",
      "12:16:09 - rank 0 YDXJ0100B.MP4 read successfully\n",
      "12:16:12 - rank 0 YDXJ0088.MP4 read successfully\n",
      "12:16:13 - rank 0 YDXJ0090B.MP4 read successfully\n",
      "12:16:14 - rank 0 YDXJ0086B.MP4 read successfully\n",
      "12:16:15 - rank 0 YDXJ0087B.MP4 read successfully\n",
      "feats torch.Size([1111, 384, 13, 13])\n",
      "12:16:21 - rank 0 starting batch 1\n",
      "12:16:25 - rank 0 YDXJ0099.MP4 read successfully\n",
      "feats torch.Size([1111, 384, 13, 13])\n",
      "12:16:35 - rank 0 starting batch 2\n",
      "12:16:36 - rank 0 YDXJ0091B.MP4 read successfully\n",
      "12:16:36 - rank 0 YDXJ0079_80.MP4 read successfully\n",
      "12:16:37 - rank 0 YDXJ0096A.MP4 read successfully\n",
      "feats torch.Size([1111, 384, 13, 13])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m frames_passed = \u001b[32m0\u001b[39m\n\u001b[32m     14\u001b[39m batches_to_proc = \u001b[32m4\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mipca_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches_to_proc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlong_vids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvid_duration_lim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_h\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_w\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mipca_videos\u001b[39m\u001b[34m(paths, rank, layer_name, model_name, model, n_components, video_type, batches_to_proc, batch_sizes, fn_list, long_vids, device, vid_duration_lim, new_h, new_w)\u001b[39m\n\u001b[32m     28\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mfeats\u001b[39m\u001b[33m\"\u001b[39m, feats.shape)\n\u001b[32m     29\u001b[39m         feats = feats.view(feats.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m).cpu().numpy()\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[43mipca\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# end with torch.no_grad():\u001b[39;00m\n\u001b[32m     32\u001b[39m \n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m#ADD FIX UNCOMMENT THIS LATER !!!!!!!\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m#joblib.dump(ipca, path) # better this or pkl?\u001b[39;00m\n\u001b[32m     35\u001b[39m print_wise(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved PCA for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, rank=rank)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/sklearn/decomposition/_incremental_pca.py:362\u001b[39m, in \u001b[36mIncrementalPCA.partial_fit\u001b[39m\u001b[34m(self, X, y, check_input)\u001b[39m\n\u001b[32m    351\u001b[39m     mean_correction = np.sqrt(\n\u001b[32m    352\u001b[39m         (\u001b[38;5;28mself\u001b[39m.n_samples_seen_ / n_total_samples) * n_samples\n\u001b[32m    353\u001b[39m     ) * (\u001b[38;5;28mself\u001b[39m.mean_ - col_batch_mean)\n\u001b[32m    354\u001b[39m     X = np.vstack(\n\u001b[32m    355\u001b[39m         (\n\u001b[32m    356\u001b[39m             \u001b[38;5;28mself\u001b[39m.singular_values_.reshape((-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)) * \u001b[38;5;28mself\u001b[39m.components_,\n\u001b[32m   (...)\u001b[39m\u001b[32m    359\u001b[39m         )\n\u001b[32m    360\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m U, S, Vt = \u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    363\u001b[39m U, Vt = svd_flip(U, Vt, u_based_decision=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    364\u001b[39m explained_variance = S**\u001b[32m2\u001b[39m / (n_total_samples - \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/scipy/_lib/_util.py:1233\u001b[39m, in \u001b[36m_apply_over_batch.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m \u001b[38;5;66;03m# Early exit if call is not batched\u001b[39;00m\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(batch_shapes):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;66;03m# Determine broadcasted batch shape\u001b[39;00m\n\u001b[32m   1236\u001b[39m batch_shape = np.broadcast_shapes(*batch_shapes)  \u001b[38;5;66;03m# Gives OK error message\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/virtual_envs/temporal_context/lib/python3.12/site-packages/scipy/linalg/_decomp_svd.py:166\u001b[39m, in \u001b[36msvd\u001b[39m\u001b[34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[39m\n\u001b[32m    162\u001b[39m lwork = _compute_lwork(gesXd_lwork, a1.shape[\u001b[32m0\u001b[39m], a1.shape[\u001b[32m1\u001b[39m],\n\u001b[32m    163\u001b[39m                        compute_uv=compute_uv, full_matrices=full_matrices)\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# perform decomposition\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m u, s, v, info = \u001b[43mgesXd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwork\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info > \u001b[32m0\u001b[39m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[33m\"\u001b[39m\u001b[33mSVD did not converge\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "video_type = \"YDX\"\n",
    "\n",
    "max_duration = 20\n",
    "\n",
    "batch_size = 1100\n",
    "fn_list = list_videos(paths, video_type)\n",
    "frames_per_vid, long_vids = get_frames_number(paths, fn_list, max_duration)\n",
    "batch_sizes = split_in_batches(frames_per_vid, batch_size)\n",
    "batch_sizes = batch_sizes[:20] #ADD TAKE OFF later\n",
    "rank = 0\n",
    "progression = 0 \n",
    "vid_duration_lim = 20 # sec\n",
    "frames_passed = 0\n",
    "batches_to_proc = 4\n",
    "\n",
    "ipca_videos(paths, rank, layer_name, model_name, model, n_components, video_type, batches_to_proc, batch_sizes, fn_list, long_vids, device, vid_duration_lim, new_h=224, new_w=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0806064",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ipca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#plt.plot(np.cumsum(ipca.explained_variance_ratio_))\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mipca\u001b[49m.components_.shape)\n",
      "\u001b[31mNameError\u001b[39m: name 'ipca' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(np.cumsum(ipca.explained_variance_ratio_))\n",
    "print(ipca.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f6d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipca_core(paths, rank, layer_name, model_name, n_components, model, loader, device):\n",
    "    save_name = (f\"imagenet_val_{model_name}_{layer_name}_pca_model_{n_components}_PCs.pkl\")\n",
    "    path = os.path.join(paths[\"results_path\"], save_name)\n",
    "    if os.path.exists(path):\n",
    "        print_wise(f\"{path} already exists\")\n",
    "    else:\n",
    "        print_wise(f\"Fitting PCA for layer: {layer_name}\", rank=rank)\n",
    "        feature_extractor = create_feature_extractor(\n",
    "            model, return_nodes=[layer_name]\n",
    "        ).to(device)\n",
    "        tmp_shape = get_layer_output_shape(feature_extractor, layer_name)\n",
    "        n_features = np.prod(tmp_shape)  # [C, H, W] -> C*H*W\n",
    "        n_components_layer = min(n_features, n_components)  # Limit to number of features\n",
    "        pca = IncrementalPCA(n_components=n_components_layer)\n",
    "        counter = 0\n",
    "        for inputs, _ in loader:\n",
    "            counter += 1\n",
    "            print_wise(f\"starting batch {counter}\", rank=rank)\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device)\n",
    "                feats = feature_extractor(inputs)[layer_name]\n",
    "                feats = feats.view(feats.size(0), -1).cpu().numpy()\n",
    "                pca.partial_fit(feats)\n",
    "\n",
    "        joblib.dump(pca, path) # better this or pkl?\n",
    "        print_wise(f\"Saved PCA for {layer_name} at {path}\", rank=rank)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temporal_context",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
