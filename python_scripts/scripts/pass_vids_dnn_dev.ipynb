{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d5259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml, sys\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from torchvision.models.feature_extraction import (\n",
    "    create_feature_extractor,\n",
    "    get_graph_node_names,\n",
    ")\n",
    "import torch \n",
    "import cv2\n",
    "\n",
    "ENV = os.getenv(\"MY_ENV\", \"dev\")\n",
    "with open(\"../../config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "paths = config[ENV][\"paths\"]\n",
    "sys.path.append(paths[\"src_path\"])\n",
    "from general_utils.utils import print_wise\n",
    "from image_processing.utils import read_video, get_video_dimensions, concatenate_frames_batch, shuffle_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc2c238",
   "metadata": {},
   "source": [
    "## Steps\n",
    "(parallel over the layers of the video)\n",
    "1. Load videos until you get approx 2 or 3 times the batch-size (or more?) (for very long vids (some of the ones with arcaro for instance - but check), just take the first 20 seconds or so...) (can we estimate the optimal order based on get_video_dimensions?) -> then shuffle, split evenly the frames and pass it to iPCA \n",
    "2. Normalize, format and shuffle them\n",
    "    - If gaze dependent, load gaze, upsample (in time) video and extract gaze-dep spatial window\n",
    "3. Compute iPCA\n",
    "4. Save eigenvectors and eigenvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc73b4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YDXJ0100A.MP4', 'YDXJ0100.MP4', 'YDXJ0090A.MP4', 'YDXJ0086A.MP4', 'YDXJ0100B.MP4', 'YDXJ0088.MP4', 'YDXJ0090B.MP4', 'YDXJ0086B.MP4', 'YDXJ0087B.MP4', 'YDXJ0099.MP4', 'YDXJ0091B.MP4', 'YDXJ0079_80.MP4', 'YDXJ0096A.MP4', 'YDXJ0097A.MP4', 'YDXJ0097B.MP4', 'YDXJ0094B.mp4', 'YDXJ0099D.MP4', 'YDXJ0094C.mp4', 'YDXJ0095E.MP4', 'YDXJ0094A.mp4', 'YDXJ0099F.MP4', 'YDXJ0099C.MP4', 'YDXJ0099B.MP4', 'YDXJ0094E.mp4', 'YDXJ0099A.MP4', 'YDXJ0088B.MP4', 'YDXJ0096.MP4', 'YDXJ0097.MP4', 'YDXJ0088A.MP4', 'YDXJ0095.MP4', 'YDXJ0094.MP4', 'YDXJ0081_82.MP4', 'YDXJ0090.MP4', 'YDXJ0091.MP4', 'YDXJ0085.MP4', 'YDXJ0085B.MP4', 'YDXJ0083_84.MP4', 'YDXJ0093.MP4', 'YDXJ0087.MP4', 'YDXJ0093A.MP4', 'YDXJ0086.MP4', 'YDXJ0092.MP4']\n"
     ]
    }
   ],
   "source": [
    "vids_dir = f\"{paths['livingstone_lab']}/Stimuli/Movies/all_videos\"\n",
    "all_files = os.listdir(vids_dir) \n",
    "fn_list = [f for f in all_files if \"YDX\" in f]\n",
    "print(fn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2752bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_per_vid = []\n",
    "long_vids = []\n",
    "for i in fn_list:\n",
    "    video_path = f\"{vids_dir}/{i}\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    _, _, n_frames = get_video_dimensions(cap)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if n_frames/fps > 20:\n",
    "        n_frames = fps*20\n",
    "        long_vids.append(True)\n",
    "    else:\n",
    "        long_vids.append(False)\n",
    "    frames_per_vid.append(n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36b7ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_frame_num = round(np.sum(frames_per_vid))\n",
    "batch_size = 1000\n",
    "counter = 0\n",
    "n_batches = round(tot_frame_num/batch_size)\n",
    "splits = np.array_split(np.arange(tot_frame_num), n_batches)\n",
    "batch_sizes = []\n",
    "for batch_idx in splits:\n",
    "    batch_sizes.append(len(batch_idx)) # stores the current batch size\n",
    "batch_sizes = np.array(batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1944d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Returns True if the list or np.array is empty, False otherwise.\n",
    "# Works for both lists and NumPy arrays.\n",
    "# \"\"\"\n",
    "# def is_empty(x):\n",
    "#     if x is None:\n",
    "#         return True\n",
    "#     try:\n",
    "#         # Works for np.array\n",
    "#         return x.size == 0\n",
    "#     except AttributeError:\n",
    "#         # If no size attribute, fallback to len() (lists, tuples)\n",
    "#         return len(x) == 0\n",
    "# # EOF\n",
    "\n",
    "# \"\"\"\n",
    "# Resize a video stored as a NumPy array of shape (n_frames, H, W, C).\n",
    "\n",
    "# INPUT:\n",
    "#     - video: np.ndarray -> (n_frames, H, W, C)\n",
    "#     - new_height: int -> desired output height\n",
    "#     - new_width: int -> desired output width\n",
    "#     - interpolation: cv2 interpolation method (default: bilinear)\n",
    "\n",
    "# OUTPUT:\n",
    "#     - resized_video: np.ndarray -> (n_frames, new_height, new_width, C)    \n",
    "# \"\"\"\n",
    "# def resize_video_array(video, new_height, new_width, interpolation=cv2.INTER_LINEAR, normalize=True):\n",
    "#     resized_video = np.stack([cv2.resize(frame, (new_width, new_height), interpolation=interpolation) for frame in video])\n",
    "#     if normalize:\n",
    "#         mean = resized_video.mean(axis=(0,1,2))\n",
    "#         std = resized_video.std(axis=(0,1,2)) + 1e-8\n",
    "#         resized_video = (resized_video - mean) / std\n",
    "#     # end if normalize:\n",
    "#     return resized_video\n",
    "# # EOF\n",
    "\n",
    "# \"\"\"\n",
    "# concatenate_frames_batch\n",
    "# Concatenate a batch of video frames from multiple videos, optionally processing only\n",
    "# the first part of long videos and keeping leftover frames from previous batches.\n",
    "\n",
    "# INPUT:\n",
    "#     - paths: dict -> paths to the video files\n",
    "#     - rank: int -> worker rank\n",
    "#     - frames_batch: list or np.ndarray -> leftover frames from previous batch\n",
    "#     - curr_video_idx: int -> current video index in fn_list\n",
    "#     - idx: int -> current batch index\n",
    "#     - batches_to_proc_togeth: int -> number of batches to process consecutively\n",
    "#     - batch_sizes: list/array -> sizes of each batch in frames\n",
    "#     - new_h, new_w: int -> target frame height and width\n",
    "#     - long_vids: list of bools -> flags indicating whether each video is long\n",
    "#     - vid_duration_lim: int (default 20) -> max duration in seconds for long videos\n",
    "#     - normalize: bool (default True) -> whether to normalize frames\n",
    "\n",
    "# OUTPUT:\n",
    "#     - frames_batch: np.ndarray -> concatenated frames for the batch\n",
    "#     - progression: int -> updated video index after processing\n",
    "\n",
    "# \"\"\"\n",
    "# def concatenate_frames_batch(paths, rank, frames_batch, curr_video_idx, curr_batch_idx, batches_to_proc_togeth, batch_sizes, new_h, new_w, long_vids, vid_duration_lim=20, normalize=True):\n",
    "#     n_batches = len(batch_sizes)\n",
    "#     idx_tot = [curr_batch_idx + i for i in range(batches_to_proc_togeth) if curr_batch_idx + i < n_batches] # takes the next $batches_to_proc frames filtering for out of range indices \n",
    "#     curr_tot_batch_size = np.sum(batch_sizes[idx_tot])\n",
    "#     print(curr_tot_batch_size)\n",
    "#     cumulative_frames_sum = 0\n",
    "#     if is_empty(frames_batch):\n",
    "#         frames_batch = [] # otherwise we have arrays of inconsistent size to concatenate\n",
    "#     else:\n",
    "#         cumulative_frames_sum += frames_batch.shape[0]\n",
    "#         frames_batch = [frames_batch] # makes it a list with all the frames remained from the previous batch (ideally we read 3 batches and shuffle)\n",
    "#     # end if frames_batch:\n",
    "#     while cumulative_frames_sum < curr_tot_batch_size:\n",
    "#         fn = fn_list[curr_video_idx]\n",
    "#         if long_vids[curr_video_idx]: # if the video is marked as long\n",
    "#             video = read_video(paths, rank, fn, vid_duration=vid_duration_lim) # if the video is too long, we just process the beginning (vid_duration_lim is in sec)\n",
    "#         else:\n",
    "#             video = read_video(paths, rank, fn, vid_duration=0)\n",
    "#         # end if long_vids[progression]: \n",
    "#         video = resize_video_array(video, new_h, new_w, normalize=False)\n",
    "#         curr_video_idx += 1\n",
    "#         curr_frames_n = video.shape[0] \n",
    "#         cumulative_frames_sum += curr_frames_n\n",
    "#         frames_batch.append(video)\n",
    "#     # end while cumulative_frames_sum < curr_tot_batch_size:\n",
    "#     frames_batch = np.concatenate(frames_batch, axis=0)\n",
    "#     return frames_batch, curr_video_idx\n",
    "# # EOF\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# shuffle_frames\n",
    "# Randomly shuffle the frames of a video array along the 0th dimension.\n",
    "\n",
    "# INPUT:\n",
    "#     - video: np.ndarray, shape (n_frames, H, W, C) -> video to shuffle\n",
    "\n",
    "# OUTPUT:\n",
    "#     - shuffled_video: np.ndarray, same shape as input -> frames randomly permuted\n",
    "# \"\"\"\n",
    "# def shuffle_frames(video):\n",
    "#     n_frames = video.shape[0] \n",
    "#     indices = np.arange(n_frames)\n",
    "#     np.random.shuffle(indices)\n",
    "#     shuffled_video = video[indices, :, :, :]\n",
    "#     return shuffled_video\n",
    "# # EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b1a6ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983\n",
      "12:34:48 - rank 0 YDXJ0100A.MP4 read successfully\n",
      "12:34:51 - rank 0 YDXJ0100.MP4 read successfully\n",
      "(544, 224, 224, 3) (983, 224, 224, 3)\n",
      "983\n",
      "12:34:52 - rank 0 YDXJ0090A.MP4 read successfully\n",
      "12:34:53 - rank 0 YDXJ0086A.MP4 read successfully\n",
      "(260, 224, 224, 3) (983, 224, 224, 3)\n",
      "983\n",
      "12:34:54 - rank 0 YDXJ0100B.MP4 read successfully\n",
      "12:34:56 - rank 0 YDXJ0088.MP4 read successfully\n",
      "(783, 224, 224, 3) (983, 224, 224, 3)\n",
      "983\n",
      "12:34:57 - rank 0 YDXJ0090B.MP4 read successfully\n",
      "(124, 224, 224, 3) (983, 224, 224, 3)\n",
      "983\n",
      "12:34:58 - rank 0 YDXJ0086B.MP4 read successfully\n",
      "12:34:59 - rank 0 YDXJ0087B.MP4 read successfully\n",
      "12:35:02 - rank 0 YDXJ0099.MP4 read successfully\n",
      "(999, 224, 224, 3) (983, 224, 224, 3)\n",
      "983\n",
      "(16, 224, 224, 3) (983, 224, 224, 3)\n",
      "983\n",
      "12:35:03 - rank 0 YDXJ0091B.MP4 read successfully\n",
      "12:35:04 - rank 0 YDXJ0079_80.MP4 read successfully\n",
      "12:35:05 - rank 0 YDXJ0096A.MP4 read successfully\n",
      "(11, 224, 224, 3) (983, 224, 224, 3)\n",
      "983\n",
      "12:35:06 - rank 0 YDXJ0097A.MP4 read successfully\n",
      "12:35:07 - rank 0 YDXJ0097B.MP4 read successfully\n",
      "12:35:08 - rank 0 YDXJ0094B.mp4 read successfully\n",
      "(38, 224, 224, 3) (983, 224, 224, 3)\n",
      "983\n",
      "12:35:10 - rank 0 YDXJ0099D.MP4 read successfully\n",
      "12:35:11 - rank 0 YDXJ0094C.mp4 read successfully\n",
      "12:35:12 - rank 0 YDXJ0095E.MP4 read successfully\n",
      "(127, 224, 224, 3) (983, 224, 224, 3)\n",
      "982\n",
      "12:35:13 - rank 0 YDXJ0094A.mp4 read successfully\n",
      "12:35:14 - rank 0 YDXJ0099F.MP4 read successfully\n",
      "12:35:15 - rank 0 YDXJ0099C.MP4 read successfully\n",
      "(218, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "12:35:16 - rank 0 YDXJ0099B.MP4 read successfully\n",
      "12:35:17 - rank 0 YDXJ0094E.mp4 read successfully\n",
      "12:35:18 - rank 0 YDXJ0099A.MP4 read successfully\n",
      "(263, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "12:35:19 - rank 0 YDXJ0088B.MP4 read successfully\n",
      "12:35:21 - rank 0 YDXJ0096.MP4 read successfully\n",
      "(438, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "12:35:24 - rank 0 YDXJ0097.MP4 read successfully\n",
      "(655, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "12:35:25 - rank 0 YDXJ0088A.MP4 read successfully\n",
      "12:35:28 - rank 0 YDXJ0095.MP4 read successfully\n",
      "(1197, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "(215, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "12:35:31 - rank 0 YDXJ0094.MP4 read successfully\n",
      "(432, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "12:35:32 - rank 0 YDXJ0081_82.MP4 read successfully\n",
      "12:35:35 - rank 0 YDXJ0090.MP4 read successfully\n",
      "(1054, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "(72, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "12:35:37 - rank 0 YDXJ0091.MP4 read successfully\n",
      "(289, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "12:35:40 - rank 0 YDXJ0085.MP4 read successfully\n",
      "(506, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "12:35:41 - rank 0 YDXJ0085B.MP4 read successfully\n",
      "12:35:42 - rank 0 YDXJ0083_84.MP4 read successfully\n",
      "(194, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "12:35:43 - rank 0 YDXJ0093.MP4 read successfully\n",
      "(22, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "12:35:46 - rank 0 YDXJ0087.MP4 read successfully\n",
      "(239, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "12:35:47 - rank 0 YDXJ0093A.MP4 read successfully\n",
      "12:35:49 - rank 0 YDXJ0086.MP4 read successfully\n",
      "(767, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "12:35:52 - rank 0 YDXJ0092.MP4 read successfully\n",
      "(984, 224, 224, 3) (982, 224, 224, 3)\n",
      "982\n",
      "(2, 224, 224, 3) (982, 224, 224, 3)\n",
      "25541 25541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = 0\n",
    "progression = 0 \n",
    "vid_duration_lim = 20 # sec\n",
    "frames_batch = [] # initialized as an empty array\n",
    "frames_passed = 0\n",
    "batches_to_proc = 1\n",
    "new_h, new_w = 224, 224\n",
    "for idx, curr_batch_size in enumerate(batch_sizes):\n",
    "    frames_batch, progression = concatenate_frames_batch(paths, rank, fn_list, frames_batch, progression, idx, batches_to_proc, batch_sizes, new_h, new_w, long_vids, vid_duration_lim)\n",
    "    frames_batch = shuffle_frames(frames_batch)\n",
    "    input = frames_batch[:curr_batch_size, :, :, :]\n",
    "    frames_batch = frames_batch[curr_batch_size:, :, :, :]\n",
    "    frames_passed += input.shape[0]\n",
    "    print(frames_batch.shape, input.shape)\n",
    "    #ADD ANN pass\n",
    "    #ADD PCA step\n",
    "\n",
    "print(tot_frame_num, frames_passed)\n",
    "tot_frame_num == frames_passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0806064",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized = a \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f6d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_layer_out_shape\n",
    "Computes the output shape (excluding batch size) of a specific layer \n",
    "from a given PyTorch feature extractor when applied to a dummy input \n",
    "image of size (1, 3, 224, 224).\n",
    "INPUT:\n",
    "- feature_extractor: torch.nn.Module -> A PyTorch model (typically a feature extractor created via torchvision.models.feature_extraction.create_feature_extractor)\n",
    "                                        which outputs a dictionary of intermediate activations.\n",
    "            \n",
    "- layer_name: str -> The name of the layer for which the output shape is desired. This must be one of the keys returned by the feature_extractor.\n",
    "\n",
    "OUTPUT:\n",
    "- tmp_shape: Tuple(Int) -> A tuple representing the shape of the output tensor from the specified layer, excluding the batch dimension. For example,\n",
    "                          (512, 7, 7) for a convolutional layer or (768,) for a transformer block.\n",
    "            \n",
    "Example Usage:\n",
    "    >>> from torchvision.models import resnet18\n",
    "    >>> from torchvision.models.feature_extraction import create_feature_extractor\n",
    "    >>> model = resnet18(pretrained=True).eval()\n",
    "    >>> feat_ext = create_feature_extractor(model, return_nodes=[\"layer1.0.relu_1\"])\n",
    "    >>> shape = get_layer_out_shape(feat_ext, \"layer1.0.relu_1\")\n",
    "    >>> print(shape)\n",
    "    (64, 56, 56)\n",
    "\"\"\"\n",
    "def get_layer_output_shape(feature_extractor, layer_name):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # thi if leave it here or not...\n",
    "    with torch.no_grad():\n",
    "        in_proxy = torch.randn(1, 3, 224, 224).to(device)\n",
    "        tmp_shape = feature_extractor(in_proxy)[layer_name].shape[1:]\n",
    "    return tmp_shape\n",
    "\n",
    "\n",
    "def ipca_core(paths, rank, layer_name, model_name, n_components, model, loader, device):\n",
    "    save_name = (f\"imagenet_val_{model_name}_{layer_name}_pca_model_{n_components}_PCs.pkl\")\n",
    "    path = os.path.join(paths[\"results_path\"], save_name)\n",
    "    if os.path.exists(path):\n",
    "        print_wise(f\"{path} already exists\")\n",
    "    else:\n",
    "        print_wise(f\"Fitting PCA for layer: {layer_name}\", rank=rank)\n",
    "        feature_extractor = create_feature_extractor(\n",
    "            model, return_nodes=[layer_name]\n",
    "        ).to(device)\n",
    "        tmp_shape = get_layer_output_shape(feature_extractor, layer_name)\n",
    "        n_features = np.prod(tmp_shape)  # [C, H, W] -> C*H*W\n",
    "        n_components_layer = min(n_features, n_components)  # Limit to number of features\n",
    "        pca = IncrementalPCA(n_components=n_components_layer)\n",
    "        counter = 0\n",
    "        for inputs, _ in loader:\n",
    "            counter += 1\n",
    "            print_wise(f\"starting batch {counter}\", rank=rank)\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device)\n",
    "                feats = feature_extractor(inputs)[layer_name]\n",
    "                feats = feats.view(feats.size(0), -1).cpu().numpy()\n",
    "                pca.partial_fit(feats)\n",
    "\n",
    "        joblib.dump(pca, path) # better this or pkl?\n",
    "        print_wise(f\"Saved PCA for {layer_name} at {path}\", rank=rank)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temporal_context",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
